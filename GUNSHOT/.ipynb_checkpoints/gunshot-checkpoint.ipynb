{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69272728-3ed5-4d88-832c-50514b97da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b098ef-cf51-4acd-bbdc-4e48e99d1542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 13:45:43.572413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743063343.606405   30378 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743063343.616735   30378 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743063343.641833   30378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743063343.641872   30378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743063343.641875   30378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743063343.641878   30378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-27 13:45:43.649645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       file_name    label\n",
      "0  Dataset/gun-voice/5 (100).wav  gunshot\n",
      "1   Dataset/gun-voice/1 (26).wav  gunshot\n",
      "2   Dataset/gun-voice/5 (20).wav  gunshot\n",
      "3   Dataset/gun-voice/5 (92).wav  gunshot\n",
      "4   Dataset/gun-voice/5 (76).wav  gunshot\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dataset/gun-voice/5 (100).wav</td>\n",
       "      <td>gunshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dataset/gun-voice/1 (26).wav</td>\n",
       "      <td>gunshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset/gun-voice/5 (20).wav</td>\n",
       "      <td>gunshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset/gun-voice/5 (92).wav</td>\n",
       "      <td>gunshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dataset/gun-voice/5 (76).wav</td>\n",
       "      <td>gunshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Dataset/not_gun-voice/social-environment-27912...</td>\n",
       "      <td>non-gunshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Dataset/not_gun-voice/forest-birds-squawking-1...</td>\n",
       "      <td>non-gunshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Dataset/not_gun-voice/nature-sounds-quiet-envi...</td>\n",
       "      <td>non-gunshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Dataset/not_gun-voice/frog-in-the-morning-2447...</td>\n",
       "      <td>non-gunshot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>Dataset/not_gun-voice/scary-wolf-growling-2240...</td>\n",
       "      <td>non-gunshot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>456 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             file_name        label\n",
       "0                        Dataset/gun-voice/5 (100).wav      gunshot\n",
       "1                         Dataset/gun-voice/1 (26).wav      gunshot\n",
       "2                         Dataset/gun-voice/5 (20).wav      gunshot\n",
       "3                         Dataset/gun-voice/5 (92).wav      gunshot\n",
       "4                         Dataset/gun-voice/5 (76).wav      gunshot\n",
       "..                                                 ...          ...\n",
       "451  Dataset/not_gun-voice/social-environment-27912...  non-gunshot\n",
       "452  Dataset/not_gun-voice/forest-birds-squawking-1...  non-gunshot\n",
       "453  Dataset/not_gun-voice/nature-sounds-quiet-envi...  non-gunshot\n",
       "454  Dataset/not_gun-voice/frog-in-the-morning-2447...  non-gunshot\n",
       "455  Dataset/not_gun-voice/scary-wolf-growling-2240...  non-gunshot\n",
       "\n",
       "[456 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Directory paths to your audio files\n",
    "gunshot_dir = 'Dataset/gun-voice'\n",
    "non_gunshot_dir = 'Dataset/not_gun-voice'\n",
    "\n",
    "# Function to create the dataset (labels and file paths)\n",
    "def create_dataset(gunshot_dir, non_gunshot_dir):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # Add gunshot files and label them as 1\n",
    "    for filename in os.listdir(gunshot_dir):\n",
    "        if filename.endswith('.wav') or filename.endswith('.mp3'):\n",
    "            file_paths.append(os.path.join(gunshot_dir, filename))\n",
    "            labels.append('gunshot')\n",
    "    \n",
    "    # Add non-gunshot files and label them as 0\n",
    "    for filename in os.listdir(non_gunshot_dir):\n",
    "        if filename.endswith('.wav') or filename.endswith('.mp3'):\n",
    "            file_paths.append(os.path.join(non_gunshot_dir, filename))\n",
    "            labels.append('non-gunshot')\n",
    "\n",
    "    return pd.DataFrame({'file_name': file_paths, 'label': labels})\n",
    "\n",
    "# Create a dataframe with file paths and labels\n",
    "df = create_dataset(gunshot_dir, non_gunshot_dir)\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "print(df.head())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53959ca-c85c-4552-993f-05f892acd708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7537226a-bd43-40eb-8b79-70b453ef2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing Completed.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Define a fixed length for MFCCs\n",
    "MAX_MFCC_LENGTH = 100  # Adjust as needed\n",
    "\n",
    "    # Pad or truncate MFCCs to MAX_MFCC_\n",
    "def extract_features(file_path, max_len=100):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Shape: (13, time_steps)\n",
    "\n",
    "    # Transpose to (time_steps, 13)\n",
    "    mfcc = mfcc.T\n",
    "\n",
    "    # Fix length (pad or truncate)\n",
    "    if mfcc.shape[0] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[0]\n",
    "        mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:max_len, :]  # Ensure shape is (max_len, 13)\n",
    "\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "# Extract features for all audio files\n",
    "X = np.array([extract_features(fp) for fp in df['file_name'].values], dtype=np.float32)\n",
    "\n",
    "# Encode labels (Gunshot/Non-Gunshot)\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['label'].values)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data Preprocessing Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f76c4238-5d58-4a35-8b16-217cde3826a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (456, 100, 13)\n",
      "X_train shape: (364, 100, 13)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([extract_features(fp) for fp in df['file_name'].values])\n",
    "\n",
    "# Print shape to debug\n",
    "print(f\"Shape of X: {X.shape}\")  # Expected (num_samples, 100, 13)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")  # Expected (num_samples, 100, 13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "601e7a89-c3e6-4a7c-97dc-8586aaff0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 13:46:24.713400: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - accuracy: 0.3525 - loss: 0.7663 - val_accuracy: 0.8804 - val_loss: 0.3967\n",
      "Epoch 2/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.9550 - loss: 0.2857 - val_accuracy: 0.8804 - val_loss: 0.3026\n",
      "Epoch 3/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9489 - loss: 0.1717 - val_accuracy: 0.8804 - val_loss: 0.2892\n",
      "Epoch 4/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9562 - loss: 0.1246 - val_accuracy: 0.8804 - val_loss: 0.2388\n",
      "Epoch 5/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9504 - loss: 0.1091 - val_accuracy: 0.8913 - val_loss: 0.2091\n",
      "Epoch 6/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9623 - loss: 0.0803 - val_accuracy: 0.9130 - val_loss: 0.1991\n",
      "Epoch 7/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9850 - loss: 0.0477 - val_accuracy: 0.9239 - val_loss: 0.1969\n",
      "Epoch 8/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.9763 - loss: 0.0500 - val_accuracy: 0.9457 - val_loss: 0.1939\n",
      "Epoch 9/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9807 - loss: 0.0406 - val_accuracy: 0.9457 - val_loss: 0.1939\n",
      "Epoch 10/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9877 - loss: 0.0355 - val_accuracy: 0.9457 - val_loss: 0.1958\n",
      "Epoch 11/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.9825 - loss: 0.0373 - val_accuracy: 0.9457 - val_loss: 0.1976\n",
      "Epoch 12/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9933 - loss: 0.0288 - val_accuracy: 0.9565 - val_loss: 0.1952\n",
      "Epoch 13/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.9967 - loss: 0.0310 - val_accuracy: 0.9348 - val_loss: 0.2109\n",
      "Epoch 14/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9919 - loss: 0.0230 - val_accuracy: 0.9565 - val_loss: 0.1920\n",
      "Epoch 15/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9964 - loss: 0.0220 - val_accuracy: 0.9457 - val_loss: 0.2108\n",
      "Epoch 16/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0181 - val_accuracy: 0.9565 - val_loss: 0.2047\n",
      "Epoch 17/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9986 - loss: 0.0215 - val_accuracy: 0.9457 - val_loss: 0.2059\n",
      "Epoch 18/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.0160 - val_accuracy: 0.9348 - val_loss: 0.2084\n",
      "Epoch 19/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0119 - val_accuracy: 0.9457 - val_loss: 0.2021\n",
      "Epoch 20/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 0.9457 - val_loss: 0.2000\n",
      "Epoch 21/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0071 - val_accuracy: 0.9565 - val_loss: 0.1962\n",
      "Epoch 22/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.9565 - val_loss: 0.2400\n",
      "Epoch 23/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9565 - val_loss: 0.2444\n",
      "Epoch 24/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9565 - val_loss: 0.2469\n",
      "Epoch 25/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9565 - val_loss: 0.2458\n",
      "Epoch 26/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.9565 - val_loss: 0.2541\n",
      "Epoch 27/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9565 - val_loss: 0.2558\n",
      "Epoch 28/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9565 - val_loss: 0.2569\n",
      "Epoch 29/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.9565 - val_loss: 0.2592\n",
      "Epoch 30/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9947 - loss: 0.0167 - val_accuracy: 0.9348 - val_loss: 0.2260\n",
      "Epoch 31/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.9948 - loss: 0.0161 - val_accuracy: 0.9457 - val_loss: 0.2247\n",
      "Epoch 32/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.9978 - loss: 0.0078 - val_accuracy: 0.9457 - val_loss: 0.2315\n",
      "Epoch 33/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.9348 - val_loss: 0.2301\n",
      "Epoch 34/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.9348 - val_loss: 0.2662\n",
      "Epoch 35/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.9967 - loss: 0.0068 - val_accuracy: 0.9565 - val_loss: 0.2238\n",
      "Epoch 36/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.9565 - val_loss: 0.2257\n",
      "Epoch 37/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9457 - val_loss: 0.2655\n",
      "Epoch 38/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.9457 - val_loss: 0.2660\n",
      "Epoch 39/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.9457 - val_loss: 0.2678\n",
      "Epoch 40/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9457 - val_loss: 0.2735\n",
      "Epoch 41/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.9457 - val_loss: 0.2756\n",
      "Epoch 42/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9457 - val_loss: 0.2766\n",
      "Epoch 43/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 0.9457 - val_loss: 0.2790\n",
      "Epoch 44/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9457 - val_loss: 0.2818\n",
      "Epoch 45/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9457 - val_loss: 0.2845\n",
      "Epoch 46/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9457 - val_loss: 0.2873\n",
      "Epoch 47/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9457 - val_loss: 0.2902\n",
      "Epoch 48/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9457 - val_loss: 0.2931\n",
      "Epoch 49/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.9457 - val_loss: 0.2944\n",
      "Epoch 50/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9457 - val_loss: 0.2960\n",
      "Epoch 51/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9457 - val_loss: 0.2998\n",
      "Epoch 52/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9457 - val_loss: 0.3018\n",
      "Epoch 53/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9457 - val_loss: 0.3023\n",
      "Epoch 54/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9457 - val_loss: 0.3051\n",
      "Epoch 55/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9457 - val_loss: 0.3071\n",
      "Epoch 56/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9457 - val_loss: 0.3087\n",
      "Epoch 57/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9457 - val_loss: 0.3107\n",
      "Epoch 58/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9457 - val_loss: 0.3133\n",
      "Epoch 59/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 9.0635e-04 - val_accuracy: 0.9457 - val_loss: 0.3153\n",
      "Epoch 60/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 9.7751e-04 - val_accuracy: 0.9457 - val_loss: 0.3172\n",
      "Epoch 61/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 9.3943e-04 - val_accuracy: 0.9457 - val_loss: 0.3194\n",
      "Epoch 62/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 9.9517e-04 - val_accuracy: 0.9457 - val_loss: 0.3210\n",
      "Epoch 63/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9457 - val_loss: 0.3224\n",
      "Epoch 64/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 7.4348e-04 - val_accuracy: 0.9457 - val_loss: 0.3236\n",
      "Epoch 65/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 8.5201e-04 - val_accuracy: 0.9457 - val_loss: 0.3247\n",
      "Epoch 66/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9457 - val_loss: 0.3261\n",
      "Epoch 67/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 9.6211e-04 - val_accuracy: 0.9457 - val_loss: 0.3277\n",
      "Epoch 68/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 9.3963e-04 - val_accuracy: 0.9457 - val_loss: 0.3292\n",
      "Epoch 69/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 7.5034e-04 - val_accuracy: 0.9457 - val_loss: 0.3310\n",
      "Epoch 70/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 7.0107e-04 - val_accuracy: 0.9457 - val_loss: 0.3331\n",
      "Epoch 71/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 7.4474e-04 - val_accuracy: 0.9457 - val_loss: 0.3351\n",
      "Epoch 72/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 9.4270e-04 - val_accuracy: 0.9457 - val_loss: 0.3376\n",
      "Epoch 73/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 8.1973e-04 - val_accuracy: 0.9457 - val_loss: 0.3392\n",
      "Epoch 74/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 6.1760e-04 - val_accuracy: 0.9457 - val_loss: 0.3398\n",
      "Epoch 75/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 7.1211e-04 - val_accuracy: 0.9457 - val_loss: 0.3411\n",
      "Epoch 76/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 5.2246e-04 - val_accuracy: 0.9457 - val_loss: 0.3434\n",
      "Epoch 77/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 4.9101e-04 - val_accuracy: 0.9457 - val_loss: 0.3454\n",
      "Epoch 78/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 6.0859e-04 - val_accuracy: 0.9457 - val_loss: 0.3473\n",
      "Epoch 79/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 5.6086e-04 - val_accuracy: 0.9457 - val_loss: 0.3500\n",
      "Epoch 80/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 7.3485e-04 - val_accuracy: 0.9457 - val_loss: 0.3536\n",
      "Epoch 81/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 5.0306e-04 - val_accuracy: 0.9348 - val_loss: 0.3623\n",
      "Epoch 82/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 4.9905e-04 - val_accuracy: 0.9348 - val_loss: 0.3706\n",
      "Epoch 83/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 4.8407e-04 - val_accuracy: 0.9348 - val_loss: 0.3730\n",
      "Epoch 84/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 5.6880e-04 - val_accuracy: 0.9457 - val_loss: 0.3637\n",
      "Epoch 85/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 4.9877e-04 - val_accuracy: 0.9457 - val_loss: 0.3627\n",
      "Epoch 86/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 5.1001e-04 - val_accuracy: 0.9457 - val_loss: 0.3608\n",
      "Epoch 87/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 6.4077e-04 - val_accuracy: 0.9457 - val_loss: 0.3609\n",
      "Epoch 88/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 5.3173e-04 - val_accuracy: 0.9457 - val_loss: 0.3624\n",
      "Epoch 89/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 5.6448e-04 - val_accuracy: 0.9457 - val_loss: 0.3652\n",
      "Epoch 90/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 5.0056e-04 - val_accuracy: 0.9457 - val_loss: 0.3663\n",
      "Epoch 91/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 5.0867e-04 - val_accuracy: 0.9457 - val_loss: 0.3685\n",
      "Epoch 92/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 4.5025e-04 - val_accuracy: 0.9457 - val_loss: 0.3703\n",
      "Epoch 93/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 5.2339e-04 - val_accuracy: 0.9457 - val_loss: 0.3731\n",
      "Epoch 94/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 2.9953e-04 - val_accuracy: 0.9457 - val_loss: 0.3711\n",
      "Epoch 95/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 2.9042e-04 - val_accuracy: 0.9457 - val_loss: 0.3719\n",
      "Epoch 96/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 4.6732e-04 - val_accuracy: 0.9457 - val_loss: 0.3737\n",
      "Epoch 97/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 4.6985e-04 - val_accuracy: 0.9457 - val_loss: 0.3760\n",
      "Epoch 98/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 4.2306e-04 - val_accuracy: 0.9457 - val_loss: 0.3798\n",
      "Epoch 99/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 4.6938e-04 - val_accuracy: 0.9348 - val_loss: 0.4105\n",
      "Epoch 100/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 3.8027e-04 - val_accuracy: 0.9348 - val_loss: 0.4176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Completed.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(100, 13), return_sequences=True))  # Use correct input shape\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "def pad_or_truncate(mfcc, max_len=100):\n",
    "    if mfcc.shape[0] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[0]\n",
    "        mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:max_len, :]\n",
    "    return mfcc\n",
    "\n",
    "X_train = np.array([pad_or_truncate(x) for x in X_train])\n",
    "X_test = np.array([pad_or_truncate(x) for x in X_test])\n",
    "\n",
    "# Reshape input data for LSTM (3D shape: samples, timesteps, features)\n",
    "#X_train = np.array([librosa.util.fix_length(x.T, size=100).T for x in X_train])\n",
    "#X_test = np.array([librosa.util.fix_length(x.T, size=100).T for x in X_test])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "model.save('gunshot_detection_model03.h5')\n",
    "\n",
    "print(\"Model Training Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8506778b-798c-48b2-86ef-f442a25c9174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9322 - loss: 0.4169\n",
      "Test Accuracy: 93.48%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67a2b12d-9b64-40bd-8615-4cea30b09cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"gunshot_detection_model03.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc36054-1d90-494c-be6c-6f700ac6ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnh3mfaxu/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnh3mfaxu/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpnh3mfaxu'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 13), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  136694180737104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694180738640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694183803792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694052997072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694052998608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694052999184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694052998992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694053000144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1743063493.541781   30378 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743063493.541823   30378 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully converted to TensorFlow Lite!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 13:48:13.542273: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpnh3mfaxu\n",
      "2025-03-27 13:48:13.543887: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-27 13:48:13.543907: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpnh3mfaxu\n",
      "I0000 00:00:1743063493.557393   30378 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "2025-03-27 13:48:13.559388: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-27 13:48:13.618111: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpnh3mfaxu\n",
      "2025-03-27 13:48:13.645021: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 102754 microseconds.\n",
      "2025-03-27 13:48:13.684632: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-03-27 13:48:13.821702: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3993] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x32xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x32xf32>>>, tensor<i32>, tensor<?x32xf32>) -> (tensor<!tf_type.variant<tensor<?x32xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<i32>, tensor<?x64xf32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x32xf32>>>, tensor<2xi32>) -> (tensor<1x?x32xf32>) : {device = \"\", num_elements = 1 : i64}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<2xi32>) -> (tensor<100x?x64xf32>) : {device = \"\", num_elements = 100 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"gunshot_detection_model03.h5\")\n",
    "\n",
    "# Create a converter object\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Enable TF Select Ops for unsupported operations (like LSTM)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Allow standard TFLite ops\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS     # Allow TF ops not natively supported in TFLite\n",
    "]\n",
    "\n",
    "# Disable experimental lowering of tensor list ops (Fixes TensorListReserve error)\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted TFLite model\n",
    "with open(\"gunshot_detection_model03.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model successfully converted to TensorFlow Lite!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00aa5283-b173-4e65-88cc-e7ce25b9577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmplcd4s139/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmplcd4s139/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmplcd4s139'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 13), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  136694255016592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694255017360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694255019280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694254773136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694254771408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694254768528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694254770448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136694254765072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model successfully converted to TensorFlow Lite!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1743063494.927595   30378 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1743063494.927614   30378 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-03-27 13:48:14.927847: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmplcd4s139\n",
      "2025-03-27 13:48:14.929452: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-03-27 13:48:14.929472: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmplcd4s139\n",
      "2025-03-27 13:48:14.943949: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-03-27 13:48:15.002113: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmplcd4s139\n",
      "2025-03-27 13:48:15.030122: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 102279 microseconds.\n",
      "2025-03-27 13:48:15.181329: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3993] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\n",
      "Details:\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x32xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListReserve(tensor<2xi32>, tensor<i32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\"}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x32xf32>>>, tensor<i32>, tensor<?x32xf32>) -> (tensor<!tf_type.variant<tensor<?x32xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListSetItem(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<i32>, tensor<?x64xf32>) -> (tensor<!tf_type.variant<tensor<?x64xf32>>>) : {device = \"\", resize_if_index_out_of_bounds = false}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x32xf32>>>, tensor<2xi32>) -> (tensor<1x?x32xf32>) : {device = \"\", num_elements = 1 : i64}\n",
      "\ttf.TensorListStack(tensor<!tf_type.variant<tensor<?x64xf32>>>, tensor<2xi32>) -> (tensor<100x?x64xf32>) : {device = \"\", num_elements = 100 : i64}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"gunshot_detection_model03.h5\")\n",
    "\n",
    "# Create a converter object\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# Enable TF Select Ops for unsupported operations (like LSTM)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Allow standard TFLite ops\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS     # Allow TF ops not natively supported in TFLite\n",
    "]\n",
    "\n",
    "# Disable experimental lowering of tensor list ops (Fixes TensorListReserve error)\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted TFLite model\n",
    "with open(\"gunshot_detection_model03.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model successfully converted to TensorFlow Lite!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2544d049-ff5b-4087-9a13-44c803887f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  GUNSHOT (Confidence: 0.9999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "INFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 19 nodes with 3 partitions.\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0.9998564245179296)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"gunshot_detection_model03.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to preprocess audio (same as during training)\n",
    "def extract_features(file_path, max_pad_len=100):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)  # Use 13 MFCCs instead of 40\n",
    "    \n",
    "    # Pad/Crop to fixed length\n",
    "    pad_width = max_pad_len - mfccs.shape[1]\n",
    "    mfccs = np.pad(mfccs, pad_width=((0, 0), (0, max(pad_width, 0))))\n",
    "    mfccs = mfccs[:, :max_pad_len]  # Crop if longer\n",
    "    \n",
    "    # Reshape to (1, 100, 13) - transpose and add batch dim\n",
    "    mfccs = mfccs.T  # Transpose to (100, 13)\n",
    "    mfccs = np.expand_dims(mfccs, axis=0)  # Add batch dim: (1, 100, 13)\n",
    "    \n",
    "    return mfccs.astype(np.float32) # Add batch dim: (1, 100, 40)\n",
    "    \n",
    "    return mfccs.astype(np.float32)\n",
    "# Example: Predict on a new audio file\n",
    "def predict_gunshot(audio_path, threshold=0.5):\n",
    "    features = extract_features(audio_path)  # Now shape (1, 100, 13)\n",
    "    \n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], features)\n",
    "    interpreter.invoke()\n",
    "    prediction = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    is_gunshot = prediction[0][0] > threshold\n",
    "    confidence = prediction[0][0] if is_gunshot else 1 - prediction[0][0]\n",
    "    label = \"NOT-GUNSHOT\" if is_gunshot else \" GUNSHOT\"\n",
    "    \n",
    "    print(f\"Prediction: {label} (Confidence: {confidence:.4f})\")\n",
    "    return is_gunshot, confidence\n",
    "predict_gunshot(\"Dataset/testing-voice/ak.mp3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac578d-2813-483c-9607-54b896c08c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
